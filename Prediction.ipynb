{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "291af19d-867d-4874-8fa1-5c53b58584fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "(4096, 20)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob as glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from utils import reject_sampling,subseting_samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "FOLDER_PATH = './Data/'\n",
    "file_paths = glob.glob(FOLDER_PATH + '/*')\n",
    "file_paths=sorted(file_paths)\n",
    "\n",
    "image_data=[];labels=[]\n",
    "for file_path in file_paths:\n",
    "    image = Image.open(file_path)\n",
    "    label=file_path.split('/')[-1].split('.bmp')[0][0]\n",
    "    image_data.append(np.array(image))\n",
    "    labels.append(label)\n",
    "\n",
    "print(len(image_data)==len(labels))\n",
    "data=np.vstack([row.reshape(-1) for row in image_data])\n",
    "A = data[:75,:].T\n",
    "B = data[75*1:75*2,:].T\n",
    "C = data[75*2:75*3,:].T\n",
    "D = data[75*3:75*4,:].T\n",
    "E = data[75*4:75*5,:].T\n",
    "F = data[75*5:75*6,:].T\n",
    "G = data[75*6:75*7,:].T\n",
    "H = data[75*7:75*8,:].T\n",
    "I = data[75*8:75*9,:].T\n",
    "J = data[75*9:75*10,:].T\n",
    "K = data[75*10:75*11,:].T\n",
    "L = data[75*11:75*12,:].T\n",
    "M = data[75*12:75*13,:].T\n",
    "#We only choose subet, cause in prcatice, no that much data\n",
    "SUB_NUM=20\n",
    "A=subseting_samples(A,SUB_NUM)\n",
    "B=subseting_samples(B,SUB_NUM)\n",
    "C=subseting_samples(C,SUB_NUM)\n",
    "D=subseting_samples(D,SUB_NUM)\n",
    "E=subseting_samples(E,SUB_NUM)\n",
    "F=subseting_samples(F,SUB_NUM)\n",
    "G=subseting_samples(G,SUB_NUM)\n",
    "H=subseting_samples(H,SUB_NUM)\n",
    "I=subseting_samples(I,SUB_NUM)\n",
    "J=subseting_samples(J,SUB_NUM)\n",
    "K=subseting_samples(K,SUB_NUM)\n",
    "L=subseting_samples(L,SUB_NUM)\n",
    "M=subseting_samples(M,SUB_NUM)\n",
    "print(A.shape==M.shape)\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "6f386ff6-4b61-488a-baf1-d64724020fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLE=20\n",
    "A_samples = reject_sampling(A,NUM_SAMPLE)\n",
    "B_samples = reject_sampling(B,NUM_SAMPLE)\n",
    "C_samples = reject_sampling(C,NUM_SAMPLE)\n",
    "D_samples = reject_sampling(D,NUM_SAMPLE)\n",
    "E_samples = reject_sampling(E,NUM_SAMPLE)\n",
    "F_samples = reject_sampling(F,NUM_SAMPLE)\n",
    "G_samples = reject_sampling(G,NUM_SAMPLE)\n",
    "H_samples = reject_sampling(H,NUM_SAMPLE)\n",
    "I_samples = reject_sampling(I,NUM_SAMPLE)\n",
    "J_samples = reject_sampling(J,NUM_SAMPLE)\n",
    "K_samples = reject_sampling(K,NUM_SAMPLE)\n",
    "L_samples = reject_sampling(L,NUM_SAMPLE)\n",
    "M_samples = reject_sampling(M,NUM_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "77c7cd78-fd10-4da6-b267-c41280392cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([A,B,C,D,E,\n",
    "                    F,G,H,I,J,\n",
    "                    K,L,M,A_samples.T, B_samples.T, C_samples.T, D_samples.T, E_samples.T,\n",
    "                    F_samples.T, G_samples.T, H_samples.T, I_samples.T, J_samples.T,\n",
    "                    K_samples.T, L_samples.T, M_samples.T],axis=1).T\n",
    "\n",
    "\n",
    "y = np.array(['A'] * A.shape[1] + ['B'] * B.shape[1] +\n",
    "             ['C'] * C.shape[1] + ['D'] * D.shape[1] +\n",
    "             ['E'] * E.shape[1] + ['F'] * F.shape[1] +\n",
    "             ['G'] * G.shape[1] + ['H'] * H.shape[1] +\n",
    "             ['I'] * I.shape[1] + ['J'] * J.shape[1] +\n",
    "             ['K'] * K.shape[1] + ['L'] * L.shape[1] +\n",
    "             ['M'] * M.shape[1]+['A'] * len(A_samples) + ['B'] * len(B_samples) +\n",
    "             ['C'] * len(C_samples) + ['D'] * len(D_samples) +\n",
    "             ['E'] * len(E_samples) + ['F'] * len(F_samples) +\n",
    "             ['G'] * len(G_samples) + ['H'] * len(H_samples) +\n",
    "             ['I'] * len(I_samples) + ['J'] * len(J_samples) +\n",
    "             ['K'] * len(K_samples) + ['L'] * len(L_samples) +\n",
    "             ['M'] * len(M_samples))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "db7b4df2-b938-4d1f-bab7-64b01e65c090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(416, 4096)\n",
      "(416,)\n",
      "(104, 4096)\n",
      "(104,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "6f9614ec-36a5-4d7c-ad63-7401fa8a75d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1444.5408325195312, Train Accuracy: 0.36538461538461536\n",
      "Epoch 2/10, Loss: 440.4207534790039, Train Accuracy: 0.6466346153846154\n",
      "Epoch 3/10, Loss: 207.89177417755127, Train Accuracy: 0.7427884615384616\n",
      "Epoch 4/10, Loss: 33.29121422767639, Train Accuracy: 0.8701923076923077\n",
      "Epoch 5/10, Loss: 10.55099630355835, Train Accuracy: 0.9447115384615384\n",
      "Epoch 6/10, Loss: 1.483999768963205, Train Accuracy: 0.9831730769230769\n",
      "Epoch 7/10, Loss: 2.712982813894996, Train Accuracy: 0.9831730769230769\n",
      "Epoch 8/10, Loss: 2.19846148416247, Train Accuracy: 0.9759615384615384\n",
      "Epoch 9/10, Loss: 0.389125603712273, Train Accuracy: 0.9951923076923077\n",
      "Epoch 10/10, Loss: 1.0114516756875667, Train Accuracy: 0.9927884615384616\n",
      "Test Accuracy: 0.9807692307692307\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_encoded, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 400\n",
    "output_size = len(label_encoder.classes_)\n",
    "model = MLP(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_accuracy = correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}, Train Accuracy: {train_accuracy}\")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_tensor, y_test_tensor = X_test_tensor.to(device), y_test_tensor.to(device)\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    correct = (predicted == y_test_tensor).sum().item()\n",
    "    total = len(y_test_tensor)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c018190e-bede-4c31-a0b8-2c9d735ac4f8",
   "metadata": {},
   "source": [
    "# No sampling what if"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "c5a18316-fd74-4571-b148-82ffe74daaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([A,B,C,D,E,\n",
    "                    F,G,H,I,J,\n",
    "                    K,L,M],axis=1).T\n",
    "\n",
    "\n",
    "y = np.array(['A'] * A.shape[1] + ['B'] * B.shape[1] +\n",
    "             ['C'] * C.shape[1] + ['D'] * D.shape[1] +\n",
    "             ['E'] * E.shape[1] + ['F'] * F.shape[1] +\n",
    "             ['G'] * G.shape[1] + ['H'] * H.shape[1] +\n",
    "             ['I'] * I.shape[1] + ['J'] * J.shape[1] +\n",
    "             ['K'] * K.shape[1] + ['L'] * L.shape[1] +\n",
    "             ['M'] * M.shape[1])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "ffc42ab7-31e3-4293-97b8-c2ce8e6eb552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208, 4096)\n",
      "(208,)\n",
      "(52, 4096)\n",
      "(52,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "4827595d-d107-41cd-beaf-f6b830c952e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1604.2152252197266, Train Accuracy: 0.08173076923076923\n",
      "Epoch 2/10, Loss: 2584.6950073242188, Train Accuracy: 0.1201923076923077\n",
      "Epoch 3/10, Loss: 2521.4771728515625, Train Accuracy: 0.11538461538461539\n",
      "Epoch 4/10, Loss: 1327.0437927246094, Train Accuracy: 0.19230769230769232\n",
      "Epoch 5/10, Loss: 493.94085693359375, Train Accuracy: 0.15384615384615385\n",
      "Epoch 6/10, Loss: 243.07913970947266, Train Accuracy: 0.28365384615384615\n",
      "Epoch 7/10, Loss: 157.50301265716553, Train Accuracy: 0.25961538461538464\n",
      "Epoch 8/10, Loss: 43.88104295730591, Train Accuracy: 0.2932692307692308\n",
      "Epoch 9/10, Loss: 28.301241636276245, Train Accuracy: 0.39903846153846156\n",
      "Epoch 10/10, Loss: 19.695741891860962, Train Accuracy: 0.5721153846153846\n",
      "Test Accuracy: 0.40384615384615385\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_encoded, dtype=torch.long)\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 400\n",
    "output_size = len(label_encoder.classes_)\n",
    "model = MLP(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_accuracy = correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}, Train Accuracy: {train_accuracy}\")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_tensor, y_test_tensor = X_test_tensor.to(device), y_test_tensor.to(device)\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    correct = (predicted == y_test_tensor).sum().item()\n",
    "    total = len(y_test_tensor)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f19fb40-d3ee-47cb-8326-a272420e11c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fa0c10-e6ab-4228-92e7-5129a124d311",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neoe",
   "language": "python",
   "name": "neoe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
